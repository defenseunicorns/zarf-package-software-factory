domain: bigbang.dev
registryCredentials:
  registry: "${ZARF_REGISTRY}"
  username: "zarf-pull"
  password: "${ZARF_REGISTRY_AUTH_PULL}"

# Openshift Container Platform Feature Toggle
openshift: false

git:
  existingSecret: "private-git-server"

networkPolicies:
  enabled: true
  # When in prod use a real CIDR. Don't do this, it isn't secure.
  controlPlaneCidr: "0.0.0.0/0"
  nodeCidr: "0.0.0.0/0"
  vpcCidr: "0.0.0.0/0"

istio:
  enabled: true
  ingressGateways:
    public-ingressgateway:
      type: "LoadBalancer"
      kubernetesResourceSpec:
      # Uncomment this section if you are on EKS and need to use an Internal Load Balancer instead of the default Classic Load Balancer
      #  serviceAnnotations:
      #    # Use an Internal Load Balancer rather than a public one
      #    service.beta.kubernetes.io/aws-load-balancer-internal: 'true'
      #    # Enable cross zone load balancing
      #    service.beta.kubernetes.io/aws-load-balancer-cross-zone-load-balancing-enabled: 'true'
        resources:
          requests:
            cpu: "100m"
            memory: "512Mi"
          limits:
            cpu: "500m"
            memory: "512Mi"
    private-ingressgateway:
      type: "LoadBalancer"
      kubernetesResourceSpec:
        serviceAnnotations: # Example for AWS internal load balancer
          service.beta.kubernetes.io/aws-load-balancer-type: nlb
          service.beta.kubernetes.io/aws-load-balancer-internal: "true"
        resources:
          requests:
            cpu: "100m"
            memory: "512Mi"
          limits:
            cpu: "500m"
            memory: "512Mi"
  values:
    istiod:
      resources:
        requests:
          cpu: "100m"
          memory: "1Gi"
        limits:
          cpu: "500m"
          memory: "1Gi"
      hpaSpec:
        maxReplicas: 1
    values:
      global:
        proxy:
          resources:
            requests:
              cpu: 100m
              memory: 256Mi
            limits:
              cpu: 100m
              memory: 256Mi

  gateways:
    public:
      ingressGateway: "public-ingressgateway"
      hosts:
        - gitlab.{{ .Values.domain }}
        - jenkins.{{ .Values.domain }}
        - jira.{{ .Values.domain }}
        - confluence.{{ .Values.domain }}
        - registry.{{ .Values.domain }}
        # DISABLE-ARTIFACTORY
        # - artifactory.{{ .Values.domain }}
        - sonarqube.{{ .Values.domain }}
        - chat.{{ .Values.domain }}
    private:
      ingressGateway: "private-ingressgateway"
      hosts:
        - neuvector.{{ .Values.domain }}
        - tracing.{{ .Values.domain }}
        - kiali.{{ .Values.domain }}
        - alertmanager.{{ .Values.domain }}
        - grafana.{{ .Values.domain }}
        - prometheus.{{ .Values.domain }}
        # Minio hosts should match hostname config in their respective values files
        # prepended with minio and minio-api like the following
        # minio.<hostname in values file> and minio-api.<hostname in values file>
        - minio.bbcore-minio.{{ .Values.domain }}
        - minio-api.bbcore-minio.{{ .Values.domain }}
        - minio.gitlab-minio.{{ .Values.domain }}
        - minio-api.gitlab-minio.{{ .Values.domain }}
        - minio.postgres-minio.{{ .Values.domain }}
        - minio-api.postgres-minio.{{ .Values.domain }}
        - minio.mattermost-minio.{{ .Values.domain }}
        - minio-api.mattermost-minio.{{ .Values.domain }}
      autoHttpRedirect:
        enabled: true

istioOperator:
  enabled: true
  values:
    operator:
      resources:
        limits:
          cpu: "500m"
          memory: "256Mi"
        requests:
          cpu: "100m"
          memory: "256Mi"

jaeger:
  enabled: true
  ingress:
    gateway: "private"
  values:
    istio:
      mtls:
        mode: STRICT
    jaeger:
      spec:
        allInOne:
          resources:
            requests:
              cpu: "100m"
              memory: "128Mi"
            limits:
              cpu: "500m"
              memory: "128Mi"
        collector:
          resources:
            requests:
              cpu: "100m"
              memory: "128Mi"
            limits:
              cpu: "500m"
              memory: "128Mi"
    resources:
      limits:
        cpu: "500m"
        memory: "128Mi"
      requests:
        cpu: "100m"
        memory: "128Mi"

kiali:
  enabled: true
  ingress:
    gateway: "private"
  values:
    istio:
      mtls:
        mode: STRICT
    resources:
      requests:
        cpu: "100m"
        memory: "512Mi"
      limits:
        cpu: "500m"
        memory: "512Mi"
    cr:
      spec:
        deployment:
          resources:
            requests:
              cpu: "200m"
              memory: "512Mi"
            limits:
              cpu: "500m"
              memory: "512Mi"

tempo:
  enabled: false

clusterAuditor:
  enabled: false

gatekeeper:
  enabled: false

elasticsearchKibana:
  enabled: false

eckOperator:
  enabled: false

fluentbit:
  enabled: false

kyverno:
  enabled: true
  values:
    replicaCount: 5
    resources:
      limits:
        cpu: "2"
        memory: 512Mi
      requests:
        cpu: "1"
        memory: 512Mi

kyvernoPolicies:
  enabled: true
  values:
    policies:
      disallow-nodeport-services:
        exclude:
          any:
          - resources:
              namespaces:
              - "zarf"
      disallow-privileged-containers:
        exclude:
          any:
          - resources:
              namespaces:
              - "local-path-storage"
      disallow-privilege-escalation:
        exclude:
          any:
          - resources:
              namespaces:
              - artifactory
              - confluence
              - gitlab
              - jira
              - sonarqube
              - mattermost
              kinds:
              - Pod
              selector:
                matchLabels:
                  application: spilo
              names:
              - "acid*"
      restrict-host-path-mount:
        exclude:
          any:
          - resources:
              namespaces:
              - "kube-system"
              - "local-path-storage"
              - "zarf"
        parameters:
          allow:
          - "/var/lib/rancher/k3s/storage/*"
          - "/var/local-path-provisioner/*"
      restrict-host-path-write:
        exclude:
          any:
          - resources:
              namespaces:
              - "kube-system"
              - "local-path-storage"
              - "zarf"
        parameters:
          allow:
          - "/var/lib/rancher/k3s/storage/*"
          - "/var/local-path-provisioner/*"
      restrict-host-path-mount-pv:
        exclude:
          any:
          - resources:
              namespaces:
              - "kube-system"
              - "local-path-storage"
              - "zarf"
        parameters:
          allow:
          - "/var/lib/rancher/k3s/storage/*"
          - "/var/local-path-provisioner/*"
      restrict-image-registries:
        skipOverlayMerge: true
        exclude:
          any:
          - resources:
              namespaces:
              - "kube-system"
              - "local-path-storage"
              - "zarf"
          # Allow the k3s loadbalancer to work
          - resources:
              namespaces:
              - istio-system
              kinds:
              - Pod
              selector:
                matchLabels:
                  svccontroller.k3s.cattle.io/svcname: public-ingressgateway
        parameters:
          allow:
            - "${ZARF_REGISTRY}"
      restrict-volume-types:
        exclude:
          any:
          - resources:
              namespaces:
              - "kube-system"
              - "local-path-storage"
              - "zarf"
      # Allow the k3s loadbalancer to work
      restrict-host-ports:
        exclude:
          any:
          - resources:
              namespaces:
              - istio-system
              kinds:
              - Pod
              selector:
                matchLabels:
                  svccontroller.k3s.cattle.io/svcname: public-ingressgateway
  # Some policies have an autogen rule that also checks upstream controllers
  # (statefulset, daemonset, deployment, etc). Adding this annotation disables
  # these autogen rules so we're only checking pods.
  postRenderers:
    - kustomize:
        patchesJson6902:
        # Required for Zarf
        - target:
            version: v1
            kind: ClusterPolicy
            name: restrict-image-registries
          patch:
          - op: add
            path: "/metadata/annotations/pod-policies.kyverno.io~1autogen-controllers"
            value: none
        # Required for the builtin k3s loadbalancer
        - target:
            version: v1
            kind: ClusterPolicy
            name: restrict-host-ports
          patch:
          - op: add
            path: "/metadata/annotations/pod-policies.kyverno.io~1autogen-controllers"
            value: none


kyvernoReporter:
  enabled: true

loki:
  enabled: true
  strategy: "scalable"
  objectStorage:
    endpoint: minio.bbcore-minio.svc.cluster.local
    region: "minio"
    # accessKey: "" -- Added in environment-bb/values.yaml
    # accessSecret: "" -- Added in environment-bb/values.yaml
    bucketNames:
      chunks: loki-logs
      ruler: loki-ruler
      admin: loki-admin
  values:
    read:
      replicas: 1
      resources:
        requests:
          cpu: 300m
          memory: 2Gi
        limits:
          cpu: 10
          memory: 3Gi
    write:
      replicas: 1
      resources:
        requests:
          cpu: 300m
          memory: 2Gi
        limits:
          cpu: 10
          memory: 3Gi
    minio:
      enabled: false
    istio:
      mtls:
        mode: STRICT
    loki:
      storage:
        s3:
          s3ForcePathStyle: true
          insecure: true
    backend:
      replicas: 1

promtail:
  enabled: true
  values:
    istio:
      mtls:
        mode: STRICT
    resources:
      limits:
        cpu: "500m"
        memory: "750Mi"
      requests:
        cpu: "100m"
        memory: "256Mi"

neuvector:
  enabled: true
  sso:
    enabled: false
  ingress:
    gateway: "private"
  values:
    controller:
      resources:
        requests:
          cpu: "1000m"
          memory: "1Gi"
        limits:
          cpu: "1000m"
          memory: "1Gi"
    enforcer:
      resources:
        requests:
          cpu: "1000m"
          memory: "2Gi"
        limits:
          cpu: "1000m"
          memory: "2Gi"
    manager:
      resources:
        requests:
          cpu: "500m"
          memory: "2Gi"
        limits:
          cpu: "500m"
          memory: "2Gi"
    cve:
      scanner:
        resources:
          requests:
            cpu: "1500m"
            memory: "2Gi"
          limits:
            cpu: "1500m"
            memory: "2Gi"
    containerd:
      enabled: true
    istio:
      mtls:
        mode: STRICT

monitoring:
  enabled: true
  ingress:
    gateway: "private"
  values:
    istio:
      mtls:
        mode: STRICT
    cleanUpgrade:
      resources:
        requests:
          memory: "128Mi"
          cpu: "100m"
        limits:
          memory: "256Mi"
          cpu: "200m"
    alertmanager:
      alertmanagerSpec:
        resources:
          limits:
            cpu: "500m"
            memory: "256Mi"
          requests:
            cpu: "100m"
            memory: "256Mi"
    grafana:
      resources:
        limits:
          cpu: "500m"
          memory: "512Mi"
        requests:
          cpu: "100m"
          memory: "256Mi"
      sidecar:
        resources:
          limits:
            cpu: "500m"
            memory: "100Mi"
          requests:
            cpu: "50m"
            memory: "50Mi"
      downloadDashboards:
        resources:
          limits:
            cpu: "20m"
            memory: "20Mi"
          requests:
            cpu: "20m"
            memory: "20Mi"
    kube-state-metrics:
      resources:
        limits:
          cpu: "500m"
          memory: "128Mi"
        requests:
          cpu: "10m"
          memory: "128Mi"
    prometheus-node-exporter:
      resources:
        limits:
          cpu: "500m"
          memory: "128Mi"
        requests:
          cpu: "100m"
          memory: "128Mi"
    prometheusOperator:
      admissionWebhooks:
        patch:
          resources:
            limits:
              cpu: "100m"
              memory: "128Mi"
            requests:
              cpu: "50m"
              memory: "128Mi"
        cleanupProxy:
          resources:
            limits:
              cpu: "100m"
              memory: "128Mi"
            requests:
              cpu: "50m"
              memory: "128Mi"
      resources:
        limits:
          cpu: "500m"
          memory: "512Mi"
        requests:
          cpu: "100m"
          memory: "512Mi"
      prometheusConfigReloader:
        resources:
          requests:
            cpu: "50m"
            memory: "128Mi"
          limits:
            cpu: "100m"
            memory: "128Mi"
    prometheus:
      prometheusSpec:
        resources:
          limits:
            cpu: "500m"
            memory: "4Gi"
          requests:
            cpu: "100m"
            memory: "4Gi"

twistlock:
  enabled: false

addons:
  mattermostOperator:
    enabled: true
    values:
      istio:
        enabled: true
  mattermost:
    enabled: true
    ingress:
      gateway: public
    # Disabled because of conflicts with postgres database
    networkPolicies:
      enabled: false
    objectStorage:
      endpoint: minio.mattermost-minio.svc.cluster.local:80
      # accessKey: "" -- Added in environment-bb/values-bigbang.enc.yaml
      # accessSecret: "" -- Added in environment-bb/values-bigbang.enc.yaml
      bucket: "mattermost-bucket"
    istio:
      enabled: true
      injection: enabled
    values:
      istio:
        enabled: true
        injection: enabled

  sonarqube:
    enabled: true
    ingress:
      gateway: public
    values:
      postgresql:
        enabled: false
      jdbcOverwrite:
        enable: true
        jdbcUrl: jdbc:postgresql://acid-sonarqube.sonarqube.svc.cluster.local:5432/sonarqube
        jdbcUsername: sonarqube
        jdbcSecretName: sonarqube.acid-sonarqube.credentials.postgresql.acid.zalan.do
        jdbcSecretPasswordKey: password
      sonarProperties:
        sonar.forceAuthentication: true
        sonar.search.javaAdditionalOpts: "-Dlog4j2.formatMsgNoLookups=true -Dcom.redhat.fips=false"
        sonar.ce.javaAdditionalOpts: "-Dcom.redhat.fips=false"
        sonar.web.javaAdditionalOpts: "-Dcom.redhat.fips=false"
      istio:
        enabled: true
        mtls:
          mode: STRICT
      resources:
        requests:
          cpu: "500m"
          memory: "1.5Gi"
        limits:
          cpu: "1"
          memory: "4Gi"
      securityContext:
        runAsUser: 1000
        runAsGroup: 1000
        fsGroup: 1000
      # Disabled because of conflicts with postgres database
      networkPolicies:
        enabled: false

  minioOperator:
    enabled: true
    values:
      operator:
        resources:
          requests:
            cpu: 200m
            memory: 256Mi
            ephemeral-storage: 500Mi
          limits:
            cpu: 200m
            memory: 256Mi
      istio:
        enabled: true
        mtls:
          mode: STRICT

  minio:
    enabled: false

  gitlab:
    enabled: true
    istio:
      injection: enabled
    objectStorage:
      type: "minio"
      endpoint: http://minio.gitlab-minio.svc.cluster.local:80
      region: "minio"
      # accessKey: "" -- Added in environment-bb/values-bigbang.enc.yaml
      # accessSecret: "" -- Added in environment-bb/values-bigbang.enc.yaml
    values:
      istio:
        mtls:
          mode: STRICT
      global:
        minio:
          enabled: false
        appConfig:
          lfs:
            bucket: gitlab-lfs
          backups:
            bucket: gitlab-backups
            tmpBucket: gitlab-backups-tmp
        registry:
          bucket: gitlab-registry
        psql:
          host: "acid-gitlab.gitlab.svc.cluster.local"
          port: 5432
          database: "postgres"
          username: "postgres"
          password:
            secret: "postgres.acid-gitlab.credentials.postgresql.acid.zalan.do"
            key: "password"
        # ## https://docs.gitlab.com/charts/installation/deployment#outgoing-email
        # ## Example Outgoing email server settings
        # smtp:
        #   enabled: true
        #   address: smtp.example.com
        #   port: 587
        #   user_name: "user@example.com"
        #   ## https://docs.gitlab.com/charts/installation/secrets#smtp-password
        #   password:
        #     secret: "gitlab-smtp-credentials"
        #     key: "password"
        #   domain: "example.com"
        #   authentication: "login"
        #   starttls_auto: true
        #   openssl_verify_mode: "peer"
        #   pool: false
        # ## https://docs.gitlab.com/charts/installation/deployment#outgoing-email
        # ## Email persona used in email sent by GitLab
        # email:
        #   from: "gitlabadmin@example.com"
        #   display_name: "GitLab Admin"
        #   reply_to: "gitlabadmin@example.com"
        #   subject_suffix: "gitlab"
        redis:
          password:
            enabled: false
          host: mymaster
          serviceName: redis
          port: 26379
          sentinels:
            - host: gitlab-redis-node-0.gitlab-redis-headless.gitlab-redis.svc.cluster.local
              port: 26379
            - host: gitlab-redis-node-1.gitlab-redis-headless.gitlab-redis.svc.cluster.local
              port: 26379
            - host: gitlab-redis-node-2.gitlab-redis-headless.gitlab-redis.svc.cluster.local
              port: 26379
      upgradeCheck:
        resources:
          requests:
            cpu: 500m
            memory: 500Mi
          limits:
            cpu: 500m
            memory: 500Mi
      redis:
        install: false
      postgresql:
        install: false
      registry:
        relativeurls: true
        storage:
          redirect:
            disable: true
        init:
          resources:
            limits:
              cpu: 200m
              memory: 200Mi
            requests:
              cpu: 200m
              memory: 200Mi
        resources:
          limits:
            cpu: 200m
            memory: 1024Mi
          requests:
            cpu: 200m
            memory: 1024Mi
      shared-secrets:
        resources:
          requests:
            cpu: 300m
            memory: 200Mi
          limits:
            cpu: 300m
            memory: 200Mi
      gitlab:
        toolbox:
          init:
            resources:
              requests:
                cpu: 200m
                memory: 200Mi
              limits:
                cpu: 200m
                memory: 200Mi
          resources:
            requests:
              cpu: 2
              memory: 3.5Gi
            limits:
              cpu: 2
              memory: 3.5Gi
          backups:
            cron:
              enabled: true
              # persistence settings for backups
              persistence:
                enabled: true
                accessMode: ReadWriteOnce
                # toolbox.persistence.size should be the same values as this
                # It needs to be sized appropriately for the user's needs. A large deployment with lots of users will
                # need enough space to temporarily store everything in GitLab including LFS, artifacts, and registry.
                # If the size of this volume is too small, the backup will fail. If the size of the restore persistence
                # below is too small, any restore attempt will fail. The space is not used for anything else, and is not
                # used unless a backup/restore is actively happening. That's wasted space that needs to be provisioned in
                # disconnected environments, but it is a benefit if EFS is being used since EFS is dynamically sized.
                # 10Gi is a demo value. Expected production values are 100Gi to 1000Gi or more depending on use case.
                # If you're over that upper value, you should consider using a different backup solution as 1+ TB backup
                # artifacts will be extremely unwieldy.
                size: 10Gi
                # storageClass: your-storage-class-here
              resources:
                requests:
                  cpu: 1
                  memory: 1Gi
                limits:
                  cpu: 10
                  memory: 2Gi
              schedule: "0 1 * * *"
            objectStorage:
              backend: s3
              config:
                secret: "gitlab-minio-s3cfg"
                key: ".s3cfg"
          # persistence settings for restore
          persistence:
            enabled: true
            accessMode: ReadWriteOnce
            size: 10Gi
            # storageClass: your-storage-class-here
        gitlab-exporter:
          init:
            resources:
              limits:
                cpu: 200m
                memory: 200Mi
              requests:
                cpu: 200m
                memory: 200Mi
          resources:
            limits:
              cpu: 150m
              memory: 200Mi
            requests:
              cpu: 150m
              memory: 200Mi
        migrations:
          init:
            resources:
              limits:
                cpu: 200m
                memory: 200Mi
              requests:
                cpu: 200m
                memory: 200Mi
          resources:
            limits:
              cpu: 500m
              memory: 1Gi
            requests:
              cpu: 500m
              memory: 1Gi
        webservice:
          init:
            resources:
              limits:
                cpu: 200m
                memory: 200Mi
              requests:
                cpu: 200m
                memory: 200Mi
          resources:
            limits:
              cpu: 600m
              memory: 2.5Gi
            requests:
              cpu: 600m
              memory: 2.5Gi
          workhorse:
            resources:
              limits:
                cpu: 600m
                memory: 2.5Gi
              requests:
                cpu: 600m
                memory: 2.5Gi
        sidekiq:
          init:
            resources:
              limits:
                cpu: 200m
                memory: 200Mi
              requests:
                cpu: 200m
                memory: 200Mi
          resources:
            requests:
              memory: 3Gi
              cpu: 1500m
            limits:
              memory: 3Gi
              cpu: 1500m
        gitaly:
          init:
            resources:
              limits:
                cpu: 200m
                memory: 200Mi
              requests:
                cpu: 200m
                memory: 200Mi
          resources:
            requests:
              cpu: 400m
              memory: 600Mi
            limits:
              cpu: 400m
              memory: 600Mi
          # Uncomment this if you want to specify a particular storage class
          # persistence:
          #   storageClass: "your-storage-class-name-here"
        gitlab-shell:
          init:
            resources:
              limits:
                cpu: 200m
                memory: 200Mi
              requests:
                cpu: 200m
                memory: 200Mi
          resources:
            limits:
              cpu: 300m
              memory: 300Mi
            requests:
              cpu: 300m
              memory: 300Mi
        praefect:
          init:
            resources:
              limits:
                cpu: 200m
                memory: 200Mi
              requests:
                cpu: 200m
                memory: 200Mi
          resources:
            requests:
              cpu: 1
              memory: 1Gi
            limits:
              cpu: 1
              memory: 1Gi
      # Disabled because of conflicts with postgres database
      networkPolicies:
        enabled: false
  gitlabRunner:
    enabled: true
    values:
      istio:
        mtls:
          mode: STRICT
      resources:
        limits:
          memory: 256Mi
          cpu: 200m
        requests:
          memory: 256Mi
          cpu: 200m
      # Disabled because of conflicts with postgres database
      networkPolicies:
        enabled: false

  nexusRepositoryManager:
    enabled: true
    # -- Base64 encoded license file.
    license_key: ""

  velero:
    enabled: false

  # Allow the metrics server (if deployed) to accept insecure connections to the
  # kubelet. This allows the metrics server to work with KinD.
  metricsServer:
    values:
      args:
        - --kubelet-insecure-tls

  keycloak:
    enabled: true
    values:
      postgresql:
        enabled: false
      extraEnv: |
        - name: DB_USER
          valueFrom:
            secretKeyRef:
              name: "postgres.acid-keycloak.credentials.postgresql.acid.zalan.do"
              key: "username"
        - name: DB_PASSWORD
          valueFrom:
            secretKeyRef:
              name: "postgres.acid-keycloak.credentials.postgresql.acid.zalan.do"
              key: "password"
        - name: DB_VENDOR
          value: "postgres"
        - name: DB_ADDR
          value: "acid-keycloak.keycloak.svc.cluster.local"
        - name: DB_PORT
          value: "5432"
        - name: DB_DATABASE
          value: "keycloak"
        - name: KC_DB_USERNAME
          valueFrom:
            secretKeyRef:
              name: "postgres.acid-keycloak.credentials.postgresql.acid.zalan.do"
              key: "username"
        - name: KC_DB_PASSWORD
          valueFrom:
            secretKeyRef:
              name: "postgres.acid-keycloak.credentials.postgresql.acid.zalan.do"
              key: "password"
        - name: KC_DB
          value: "postgres"
        - name: KC_DB_URL_HOST
          value: "acid-keycloak.keycloak.svc.cluster.local"
        - name: KC_DB_URL_PORT
          value: "5432"
        - name: KC_DB_URL_DATABASE
          value: "keycloak"
