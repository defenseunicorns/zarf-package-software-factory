# yaml-language-server: $schema=https://raw.githubusercontent.com/defenseunicorns/zarf/v0.25.2/zarf.schema.json
kind: ZarfPackageConfig
metadata:
  name: di2me-artifactory-restorable-backup
  description: Zarf package for backing up and restoring Artifactory in a DI2-ME environment. On package create, the backup will be pulled out of the cluster and added to the package. On deploy, All backup files will be placed in the local directory. Perform the restore by deploying the optional component.
  url: https://github.com/defenseunicorns/zarf-package-software-factory/tree/main/backup-and-restore/gitlab
  version: "###ZARF_PKG_VAR_BACKUP_TIMESTAMP###"

components:
  - name: preflight-checks
    description: "Run preflight checks"
    required: true
    actions:
      onCreate:
        defaults:
          maxRetries: 5
        before:
          - cmd: if ! command -v kubectl; then echo "kubectl is required for this package" >&2; exit 1; fi
      onDeploy:
        defaults:
          maxRetries: 5
        before:
          - cmd: if ! command -v flux; then echo "flux cli is required for this package" >&2; exit 1; fi

  - name: variable-collection
    description: "Collect information from cluster to be used later"
    required: true
    actions:
      onCreate:
        defaults:
          maxRetries: 5
        before:
          # Create tmp folder for writing files during create
          - cmd: mkdir -p tmp

          # Get postgresql cluster uid and store in a file since variables don't work during package create
          - cmd: kubectl get postgresql -n artifactory acid-artifactory -o jsonpath='{.metadata.uid}' > tmp/postgres-cluster-uid

  - name: artifactory-backup-database
    description: "On package create, grabs the backup from the cluster and adds it to the package. On deploy, adds the files to the current directory."
    required: true
    actions:
      onCreate:
        defaults:
          maxRetries: 5
        before:
          # Force a backup to be ran from the postgres pod to get the latest data
          - cmd: bash -c 'for (( c=0; c<$(kubectl get Postgresql -n artifactory acid-artifactory -o jsonpath="{.spec.numberOfInstances}"); c++ )); do kubectl exec -i -n artifactory acid-artifactory-$c -c postgres -- envdir "/run/etc/wal-e.d/env" /scripts/postgres_backup.sh "/home/postgres/pgdata/pgroot/data"; done'

          # Get the name of the latest backup
          - cmd: kubectl exec -i -n artifactory acid-artifactory-0 -c postgres -- bash -c 'envdir "/run/etc/wal-e.d/env" wal-g backup-list | tail -n 1 | cut -d " " -f1' > tmp/latest-backup-name


          # Create a pod that can run binaries and temporarily store files
          - cmd: kubectl apply -f files/pod.yaml

          # Wait for backup-restore-tools pod to be ready
          - cmd: kubectl wait --for=jsonpath='{.status.phase}'=Running -n artifactory pod/backup-restore-tools


          # Copy the minio client binary to the backup-restore-tools pod
          - cmd: kubectl cp files/mc artifactory/backup-restore-tools:/mounts/workspace/mc

          # Set credentials and url for the minio client
          - cmd: sleep 2s; kubectl exec -i -n artifactory backup-restore-tools -c toolbox -- bash -c "cd /mounts/workspace; HOME=/mounts/workspace; /mounts/workspace/mc alias set postgres http://minio.postgres-minio.svc.cluster.local:80 $(kubectl get secret minio-user-creds -n postgres-minio -o jsonpath='{.data.CONSOLE_ACCESS_KEY}' | base64 -d) $(kubectl get secret minio-user-creds -n postgres-minio -o jsonpath='{.data.CONSOLE_SECRET_KEY}' | base64 -d)"


          # Copy the backup folder from the bucket to the backup-restore-tools pod TODO add logic around only downloading the latest backup if wanted
          - cmd: sleep 2s; kubectl exec -i -n artifactory backup-restore-tools -c toolbox -- bash -c "cd /mounts/workspace; HOME=/mounts/workspace; /mounts/workspace/mc cp postgres/postgres-backups/spilo/acid-artifactory/$(cat tmp/postgres-cluster-uid)/ /mounts/workspace/backup --recursive"

          # Tar the backup folder in the backup-restore-tools pod
          - cmd: kubectl exec -i -n artifactory backup-restore-tools -c toolbox -- bash -c 'cd /mounts/workspace/backup; tar -cvf ../db-backup.tar wal'

          # Copy the db-backup.tar file from the backup-restore-tools pod to the local machine
          - cmd: kubectl cp artifactory/backup-restore-tools:/mounts/workspace/db-backup.tar tmp/db-backup.tar


          #  Tar the artifactory folder in the backup-restore-tools pod
          - cmd: kubectl exec -i -n artifactory backup-restore-tools -c toolbox -- bash -c 'cd /mounts; tar -cvf /mounts/workspace/artifactory-backup.tar artifactory'

          # Copy the artifactory-backup.tar file from the backup-restore-tools pod to the local machine
          - cmd: kubectl cp artifactory/backup-restore-tools:/mounts/workspace/artifactory-backup.tar tmp/artifactory-backup.tar
        onFailure:
          - cmd: kubectl delete -f files/pod.yaml --wait=true --force=true --grace-period=0
          - cmd: rm -rf tmp
        after:
          - cmd: rm -rf tmp
          - cmd: kubectl delete -f files/pod.yaml --wait=true --force=true --grace-period=0
    files:
      - source: tmp/db-backup.tar
        target: db-backup.tar
      - source: tmp/artifactory-backup.tar
        target: artifactory-backup.tar
      - source: tmp/postgres-cluster-uid
        target: postgres-cluster-uid
      - source: tmp/latest-backup-name
        target: latest-backup-name
        executable: true
      - source: files/mc
        target: mc
        executable: true
      - source: files/pod.yaml
        target: pod.yaml

  - name: warning-downtime-begin-restore
    required: false
    description: "WARNING: This will cause downtime -- Start restore actions. This action cannot be cancelled."
    actions:
      onDeploy:
        defaults:
          maxRetries: 5
        after:
          # Suspend top level kustomization that controls helmreleases
          - cmd: flux suspend kustomization softwarefactoryaddons

          # Scale artifactory to 0 to allow for restore
          - cmd: kubectl scale --replicas=0 -n artifactory statefulset/artifactory
          - cmd: kubectl wait --for=jsonpath='{.status.availableReplicas}'=0 -n artifactory statefulset/artifactory --timeout=300s

          # Delete the postgresql helmrelease therefore deleting the postgresql cluster
          - cmd: if kubectl get -n softwarefactoryaddons hr artifactory-database; then kubectl delete -n softwarefactoryaddons hr artifactory-database; fi

          # Create a pod that can run binaries and temporarily store files
          - cmd: kubectl apply -f pod.yaml

          # Wait for backup-restore-tools pod to be ready
          - cmd: kubectl wait --for=jsonpath='{.status.phase}'=Running -n artifactory pod/backup-restore-tools --timeout=300s


          # Copy the minio client binary to the backup-restore-tools pod
          - cmd: kubectl cp mc artifactory/backup-restore-tools:/mounts/workspace/mc

          # Set credentials and url for the minio client
          - cmd: sleep 2s; kubectl exec -i -n artifactory backup-restore-tools -c toolbox -- bash -c "cd /mounts/workspace; HOME=/mounts/workspace; /mounts/workspace/mc alias set postgres http://minio.postgres-minio.svc.cluster.local:80 $(kubectl get secret minio-user-creds -n postgres-minio -o jsonpath='{.data.CONSOLE_ACCESS_KEY}' | base64 -d) $(kubectl get secret minio-user-creds -n postgres-minio -o jsonpath='{.data.CONSOLE_SECRET_KEY}' | base64 -d)"


          # Copy the db-backup.tar file from the local machine to the backup-restore-tools pod
          - cmd: kubectl cp db-backup.tar artifactory/backup-restore-tools:/mounts/workspace/db-backup.tar

          # Untar the db-backup inside the pod
          - cmd: kubectl exec -i -n artifactory backup-restore-tools -c toolbox -- bash -c 'cd /mounts/workspace; tar -xf db-backup.tar'

          # Write backup timestamp to file
          - cmd: kubectl exec -i -n artifactory backup-restore-tools -c toolbox -- bash -c "jq '.start_time' /mounts/workspace/wal/11/basebackups_005/$(cat latest-backup-name)/metadata.json | tr -d '\"'" > latest-backup-timestamp

          # Copy the copy the backup data into the minio bucket
          - cmd: sleep 2s; kubectl exec -i -n artifactory backup-restore-tools -c toolbox -- bash -c "cd /mounts/workspace; HOME=/mounts/workspace; /mounts/workspace/mc cp /mounts/workspace/wal postgres/postgres-backups/spilo/acid-artifactory/$(cat postgres-cluster-uid)/ --recursive"


          # Create temp artifactory folder to untar the backup into
          - cmd: kubectl exec -i -n artifactory backup-restore-tools -c toolbox -- mkdir /mounts/workspace/artifactory-tmp

          # Copy the artifactory-backup.tar file from the local machine to the backup-restore-tools pod
          - cmd: kubectl cp artifactory-backup.tar artifactory/backup-restore-tools:/mounts/workspace/artifactory-backup.tar

          # Untar the artifactory-backup inside the pod to the temp folder
          - cmd: kubectl exec -i -n artifactory backup-restore-tools -c toolbox -- bash -c 'cd /mounts/workspace; tar -xf artifactory-backup.tar -C /mounts/workspace/artifactory-tmp'

          # Delete everything in the existing artifactory folder
          - cmd: kubectl exec -i -n artifactory backup-restore-tools -c toolbox -- bash -c 'rm -rf /mounts/workspace/artifactory/*'

          # Copy the contents of the backup from the temp folder over the existing artifactory folder
          - cmd: kubectl exec -i -n artifactory backup-restore-tools -c toolbox -- bash -c 'cp -R /mounts/workspace/artifactory-tmp/artifactory/* /mounts/artifactory'


          # Delete the backup-restore-tools pod
          - cmd: kubectl delete -f pod.yaml --wait=true --force=true --grace-period=0
        onFailure:
          - cmd: kubectl delete -f pod.yaml --wait=true --force=true --grace-period=0
